{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445e0395-20b7-4507-bfd8-fda338baddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import ssl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161df74a-0c8c-4266-81d3-11387740cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuram contextul SSL pentru a permite accesul la site-urile care au certificate nesigure\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "# Descarcam tokenizerul 'punkt' necesar pentru evaluarea BLEU\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.data.path.append(\"C:/Users/ungur/AppData/Roaming/nltk_data\")\n",
    "tokenizer_bleu = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70af316e-ac7f-42b2-9637-0e0a2c64f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incarcare sonete Shakespeare din web. Extrage paragrafele HTML si le converteste intr-o lista de liste de versuri\n",
    "# Pastreaza doar acele paragrafe care contin cel putin 3 versuri\n",
    "\n",
    "def incarca_sonete():\n",
    "    raspuns = requests.get(\"https://www.shakespeares-sonnets.com/all.php\", headers={'User-Agent': 'Mozilla/5.0'}, verify=False)\n",
    "    soup = BeautifulSoup(raspuns.text, 'html.parser')\n",
    "    paragrafe = soup.find_all('p')\n",
    "    sonete = []\n",
    "    for paragraf in paragrafe:\n",
    "        text = paragraf.get_text(separator=\"\\n\").replace(\"\\r\", \"\").strip()\n",
    "        linii = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "        if len(linii) >= 3:\n",
    "            sonete.append(linii)\n",
    "    return sonete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c309639b-5949-40ba-806e-939b8fff1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impartire poezii in strofe de 3 versuri\n",
    "\n",
    "def extrage_strofe(poem):\n",
    "    return [poem[i:i+3] for i in range(0, len(poem), 3) if len(poem[i:i+3]) == 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02da62d-b836-4ae9-83c3-87127f88b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluare model pe baza scorului BLEU. Pentru fiecare strofa se ia primul vers (restul fiind \"pierdut\" dupa inundatie), \n",
    "# se genereaza 1-3 versuri noi, se compara versurile generate cu cele reale (care au fost pastrate pentru comparatie),\n",
    "# se calculeaza scorul BLEU\n",
    "\n",
    "def evalueaza_model(model, tokenizer, lista_strofe, lista_parametri, nume_model=\"GPT2\", num_strofe=2):\n",
    "    functie_smooth = SmoothingFunction().method4\n",
    "    rezultate = []\n",
    "\n",
    "    for i, (prim_vers, versuri_referinta) in enumerate(lista_strofe[:num_strofe]):\n",
    "        for max_tokeni, temperatura, top_p in lista_parametri:\n",
    "            # Pregatim input-ul pentru model\n",
    "            intrare = tokenizer.encode(prim_vers + \"\\n\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # Generam continuarea strofei\n",
    "            attention_mask = torch.ones_like(intrare)\n",
    "            iesire = model.generate(\n",
    "                intrare,\n",
    "                max_length=intrare.shape[1] + max_tokeni,\n",
    "                temperature=temperatura,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            # Extragem textul generat si pastram doar primele 3 versuri\n",
    "            text_generat = tokenizer.decode(iesire[0], skip_special_tokens=True)[len(prim_vers):].strip()\n",
    "            linii_generate = [l for l in text_generat.split(\"\\n\") if l][:3]\n",
    "\n",
    "            # Tokenizam atat referinta cat si generarea pentru calcul BLEU\n",
    "            ref_tokeni = [tokenizer_bleu.tokenize(\" \".join(versuri_referinta).lower())]\n",
    "            cand_tokeni = tokenizer_bleu.tokenize(\" \".join(linii_generate).lower())\n",
    "            scor = sentence_bleu(ref_tokeni, cand_tokeni, smoothing_function=functie_smooth)\n",
    "\n",
    "            # Afisam comparatia si scorul BLEU\n",
    "            print(f\"Scor BLEU: {scor:.4f}\")\n",
    "\n",
    "            rezultate.append({\n",
    "                'model': nume_model,\n",
    "                'prompt': prim_vers,\n",
    "                'original': versuri_referinta,\n",
    "                'generat': linii_generate,\n",
    "                'bleu': scor,\n",
    "                'max_tokeni': max_tokeni,\n",
    "                'temperatura': temperatura,\n",
    "                'top_p': top_p\n",
    "            })\n",
    "    return rezultate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4ac714-a304-450a-8e15-03714ddf9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testare prompturi in diferite limbi. Genereaza un text pe baza lui folosind modelul si tokenizerul specificat\n",
    "\n",
    "def test_limba_prompt(model, tokenizer, prompt, descriere):\n",
    "    print(f\"\\n Test {descriere}\")\n",
    "    intrare = tokenizer.encode(prompt + \"\\n\", return_tensors=\"pt\").to(model.device)\n",
    "    iesire = model.generate(intrare, max_length=intrare.shape[1] + 60, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "    rezultat = tokenizer.decode(iesire[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    print(f\"Prompt: {prompt}\\nGenerat:\\n{rezultat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a8f53b-0879-42cf-9bb4-54450ca292cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereaza o poezie cu stil pastel, pornind de la un vers de inceput. Adauga o indicatie stilistica ca hint textual pentru model\n",
    "\n",
    "def genereaza_pastel(prompt, stil=\"pastoral scene with nature\", max_tokeni=60):\n",
    "    seed = prompt + f\"\\n[{stil}]\"\n",
    "    intrare = poet_tokenizer.encode(seed, return_tensors=\"pt\").to(poet_model.device)\n",
    "    iesire = poet_model.generate(intrare, max_length=intrare.shape[1]+max_tokeni, temperature=0.6, top_p=0.85, do_sample=True)\n",
    "    rezultat = poet_tokenizer.decode(iesire[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    print(f\"\\n Stil pastel:\\n{rezultat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6634720c-58be-4730-9649-4cd3571c21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incarca sonetele Shakespeare de pe web. Extrage strofe de 3 versuri. Incarca 2 modele: GPT2 (generalist) si unul poetic antrenat pe poezii\n",
    "# Incarca tokenizatoarele corespunzatoare\n",
    "\n",
    "def init_model_si_date():\n",
    "    global gpt2_model, gpt2_tokenizer, poet_model, poet_tokenizer, strofe\n",
    "    response = requests.get(\"https://www.shakespeares-sonnets.com/all.php\", headers={'User-Agent': 'Mozilla/5.0'}, verify=False)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragrafe = soup.find_all('p')\n",
    "    sonete = []\n",
    "    for paragraf in paragrafe:\n",
    "        text = paragraf.get_text(separator=\"\\n\").replace(\"\\r\", \"\").strip()\n",
    "        linii = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "        if len(linii) >= 3:\n",
    "            sonete.append(linii)\n",
    "    strofe = [(s[0], s[1:]) for poem in sonete for s in [poem[i:i+3] for i in range(0, len(poem), 3) if len(poem[i:i+3]) == 3]]\n",
    "\n",
    "    # Model generalist\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    gpt2_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        gpt2_model.to(\"cuda\")\n",
    "\n",
    "    # Model poetic\n",
    "    poet_tokenizer = AutoTokenizer.from_pretrained(\"ayazfau/GPT2-124M-poetry-RL\")\n",
    "    poet_model = AutoModelForCausalLM.from_pretrained(\"ayazfau/GPT2-124M-poetry-RL\")\n",
    "    poet_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        poet_model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad80461-04f0-4574-9978-571a4bdbfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analiza_calitativa_demo(rezultate_gpt2, rezultate_poet):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(rezultate_gpt2 + rezultate_poet)\n",
    "\n",
    "    # Diferente scor BLEU\n",
    "    scoruri_c1 = df.groupby(\"model\")[\"bleu\"].mean().reset_index()\n",
    "    print(\"\\nScor BLEU mediu pentru fiecare model:\")\n",
    "    for _, row in scoruri_c1.iterrows():\n",
    "        print(f\"â†’ {row['model']:<12s} | BLEU: {row['bleu']:.4f}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(scoruri_c1['model'], scoruri_c1['bleu'], color=['purple', 'pink'])\n",
    "    plt.title(\" Comparatie BLEU mediu\")\n",
    "    plt.ylabel(\"BLEU\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "    # Daca versurile din prompt sunt in limba engleza, romana, romana si corpusul de antrenare este in limba engleza\n",
    "    prompts = {\n",
    "        \"EN prompt\": \"Is this a time to be cloudy and sad,\",\n",
    "        \"RO prompt\": \"Te uita cum ninge decembre...\",\n",
    "        \"RO prompt pe model EN\": \"Te uita cum ninge decembre...\"\n",
    "    }\n",
    "\n",
    "    for label, prompt in prompts.items():\n",
    "        print(f\"\\n {label}\")\n",
    "        \n",
    "        inp_gpt2 = gpt2_tokenizer.encode(prompt + \"\\n\", return_tensors=\"pt\").to(gpt2_model.device)\n",
    "        out_gpt2 = gpt2_model.generate(inp_gpt2, max_length=inp_gpt2.shape[1]+50, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=gpt2_tokenizer.eos_token_id)\n",
    "        rezultat_gpt2 = gpt2_tokenizer.decode(out_gpt2[0], skip_special_tokens=True).replace('\\n', ' ')\n",
    "        print(f\"â†’ GPT-2: {rezultat_gpt2.strip()}\")\n",
    "\n",
    "        inp_poet = poet_tokenizer.encode(prompt + \"\\n\", return_tensors=\"pt\").to(poet_model.device)\n",
    "        out_poet = poet_model.generate(inp_poet, max_length=inp_poet.shape[1]+50, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=poet_tokenizer.eos_token_id)\n",
    "        rezultat_poet = poet_tokenizer.decode(out_poet[0], skip_special_tokens=True).replace('\\n', ' ')\n",
    "        print(f\"â†’ Poetic-GPT2: {rezultat_poet.strip()}\")\n",
    "\n",
    "    # Personalizare stil pastel prin prompting\n",
    "    pastel_prompt = \"In the light of sunset, the field blooms in silence\"\n",
    "    styled_prompt = pastel_prompt + \"\\n[style: pastel nature imagery]\"\n",
    "    inp_pastel = poet_tokenizer.encode(styled_prompt, return_tensors=\"pt\").to(poet_model.device)\n",
    "    out_pastel = poet_model.generate(inp_pastel, max_length=inp_pastel.shape[1]+50, do_sample=True, temperature=0.6, top_p=0.85, pad_token_id=poet_tokenizer.eos_token_id)\n",
    "    rezultat = poet_tokenizer.decode(out_pastel[0], skip_special_tokens=True).replace('\\n', ' ')\n",
    "    print(f\"\\n Stil pastel: {rezultat.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba3886-0e33-470d-8314-44a262b40a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulare_lab():\n",
    "    init_model_si_date()\n",
    "    parametri = [(50, 0.6, 0.85), (60, 0.7, 0.9), (50, 0.8, 0.95)]\n",
    "\n",
    "    # CERINTA a \n",
    "    print(\"\\n Evaluare model generalist GPT2\")\n",
    "    rezultate_gpt2 = evalueaza_model(gpt2_model, gpt2_tokenizer, strofe, parametri, \"GPT2\", num_strofe=5)\n",
    "\n",
    "    # CERINTA b\n",
    "    print(\"\\n Evaluare model poetic specializat\")\n",
    "    rezultate_poet = evalueaza_model(poet_model, poet_tokenizer, strofe, parametri, \"Poetic-GPT2\", num_strofe=5)\n",
    "\n",
    "    # Afisare scoruri BLEU comparativ\n",
    "    df_rezultate = pd.DataFrame(rezultate_gpt2 + rezultate_poet)\n",
    "    avg_bleu = df_rezultate.groupby(\"model\")[\"bleu\"].mean().reset_index()\n",
    "    print(\"\\n Scoruri BLEU medii pe model:\")\n",
    "    for _, row in avg_bleu.iterrows():\n",
    "        print(f\"â†’ {row['model']:12s}: {row['bleu']:.4f}\")\n",
    "\n",
    "    # Grafic BLEU\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(avg_bleu['model'], avg_bleu['bleu'], color=['purple', 'pink'])\n",
    "    plt.title(\"Scor BLEU mediu pe model\")\n",
    "    plt.ylabel(\"BLEU\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # CERINTA c\n",
    "    try:\n",
    "        rezultate_gpt2 = evalueaza_model(gpt2_model, gpt2_tokenizer, strofe, [(60, 0.7, 0.9)], \"GPT2\")\n",
    "        rezultate_poet = evalueaza_model(poet_model, poet_tokenizer, strofe, [(60, 0.7, 0.9)], \"Poetic-GPT2\")\n",
    "        analiza_calitativa_demo(rezultate_gpt2, rezultate_poet)\n",
    "\n",
    "        # Salvam poezia cu BLEU maxim\n",
    "        ofera_max = max(rezultate_poet, key=lambda x: x['bleu'])\n",
    "        with open(\"poezie_preferata.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Prompt:\\n\" + ofera_max['prompt'] + \"\\n\\nGenerat:\\n\" + \"\\n\".join(ofera_max['generat']))\n",
    "        print(\"Poezia a fost salvata in 'poezie_preferata.txt'\")\n",
    "    except Exception as e:\n",
    "        print(\"Eroare la generare demo analiza c:\", e)\n",
    "rulare_lab()\n",
    "\n",
    "\n",
    "# CERINTA c.1: Diferentele de calitate intre GPT2 generalist si modelul poetic.\n",
    "# Modelul poetic produce versuri mai stilizate si apropiate de stilul poetic,\n",
    "# avand scoruri BLEU mai bune comparativ cu modelul generalist.\n",
    "\n",
    "# CERINTA c.2: Prompt in limba engleza.\n",
    "# Atat modelul generalist, cat si cel poetic genereaza texte coerente si fluente\n",
    "# atunci cand promptul este in limba engleza, limbajul lor principala.\n",
    "\n",
    "# CERINTA c.3: Prompt in limba romana.\n",
    "# Modelele au dificultati in a genera texte corecte in romana,\n",
    "# deoarece nu sunt antrenate pe acest limbaj, rezultand texte incoerente.\n",
    "\n",
    "# CERINTA c.4: Prompt in romana, model antrenat pe engleza.\n",
    "# Cand promptul este in romana dar modelele sunt antrenate in engleza,\n",
    "# generarea ramane preponderent in engleza, afectand calitatea textului.\n",
    "\n",
    "# CERINTA c.5: Personalizarea stilului pastel.\n",
    "# Stilul poetic poate fi directionat folosind prompt engineering,\n",
    "# adaugand indicatii stilistice in prompt pentru a genera imagini tematice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0876735-87e3-412b-ad71-043fafabc335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
